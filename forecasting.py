# -*- coding: utf-8 -*-
"""Forecasting .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FGITkiAuhBgYuQJcg77wndkAWVaOpZ7J
"""

import numpy as np  # for numerical operations
import pandas as pd  # for data handling
import matplotlib.pyplot as plt  # for plotting
import statsmodels.api as sm  # for statistical models
from statsmodels.tsa.arima.model import ARIMA  # for ARIMA model
from statsmodels.tsa.stattools import adfuller  # for stationarity test
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf  # for ACF and PACF plots
from sklearn.metrics import mean_squared_error  # for error calculation
from math import sqrt  # for square root

df = pd.read_csv('/content/drive/MyDrive/ARIMA/perrin-freres-monthly-champagne-.csv')

df.head()

df.columns = ["Month", "Sales"] # Rename columns for clarity

df.head()

df.tail()

df.isna().sum()

df.drop([105, 106], axis=0, inplace=True) #Drop NaN values

df.tail()

df.info()

df['Month'] = pd.to_datetime(df['Month'])

df.info()

df['Sales'] = pd.to_numeric(df['Sales'], errors='coerce') # convert Sales column to numeric, set errors to NaN

df = df.dropna()

df['Sales'].plot(title="Original Time Series") #seasonal data

def adfuller_test(series):
    result = adfuller(series.dropna()) # run ADF test after dropping NaNs
    print('ADF Statistic:', result[0]) # print ADF test statistic
    print('p-value:', result[1])       # print p-value
    if result[1] <= 0.05:
        print("Data is stationary")
    else:
        print("Data is non-stationary")

adfuller_test(df['Sales'])

# remove seasonality by differencing with lag 12
 # lag- no. of time steps shifted back in time
df['Seasonal First Difference'] = df['Sales'] - df['Sales'].shift(12)

df.head(14)

plot_acf(df['Seasonal First Difference'].dropna(), lags=40)  # drop NaNs, plot autocorrelation for 40 lags , ACF → q
plot_pacf(df['Seasonal First Difference'].dropna(), lags=40) # drop NaNs, plot partial autocorrelation for 40 lags,PACF → p
plt.show()

train = df.iloc[:100]
test = df.iloc[100:]

#ARIMA
model_arima = ARIMA(train['Sales'], order=(1,1,1)) # order p=d=q=1
fit_arima = model_arima.fit() #fit the model to training data
forecast_arima = fit_arima.predict(start=100, end=103, dynamic=True) # Forecast from index 100 to 103 using ARIMA

#SARIMA
model_sarima = sm.tsa.statespace.SARIMAX(train['Sales'], order=(1,1,1), seasonal_order=(1,1,1,12)) #seasonal component (P=1, D=1, Q=1, S=12)
fit_sarima = model_sarima.fit() #fit the model to testing model
forecast_sarima = fit_sarima.predict(start=100, end=103, dynamic=True) # Forecast from index 100 to 103 using SARIMA

# Create exogenous variable
exog = np.arange(len(df)).reshape(-1, 1)

# Split exogenous data into train and test
train_exog = exog[:100]
test_exog = exog[100:]

# Define SARIMAX model
model_sarimax = sm.tsa.statespace.SARIMAX(train['Sales'], exog=train_exog, order=(1,1,1), seasonal_order=(1,1,1,12))

# Fit the model
fit_sarimax = model_sarimax.fit()

# Forecast next 4 values
forecast_sarimax = fit_sarimax.predict(start=100, end=103, exog=test_exog[:4], dynamic=True)

plt.figure(figsize=(12,6))
plt.plot(test.index[:4], test['Sales'][:4], label='Actual')
plt.plot(test.index[:4], forecast_arima, label='ARIMA')
plt.plot(test.index[:4], forecast_sarima, label='SARIMA')
plt.plot(test.index[:4], forecast_sarimax, label='SARIMAX')
plt.legend()
plt.title("Forecast Comparison")
plt.show()

from sklearn.metrics import mean_squared_error
from math import sqrt

# Use [:4] to match 4-step forecast length
rmse_arima = sqrt(mean_squared_error(test['Sales'][:4], forecast_arima))
rmse_sarima = sqrt(mean_squared_error(test['Sales'][:4], forecast_sarima))
rmse_sarimax = sqrt(mean_squared_error(test['Sales'][:4], forecast_sarimax))

print(f"ARIMA RMSE: {rmse_arima:.2f}")
print(f"SARIMA RMSE: {rmse_sarima:.2f}")
print(f"SARIMAX RMSE: {rmse_sarimax:.2f}")

# OBSERVATIONS
#ARIMA	- High error → not capturing seasonality/exogenous factors well
#SARIMA	- Much better → accounts for seasonality
#SARIMAX- Similar to SARIMA → exogenous variable had little to no extra impact